{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85b9cad9-14cc-4145-a71a-6fdfa7b9b044",
   "metadata": {},
   "source": [
    "# BLEU Score for Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d152300f-00fd-4f31-bb2c-a776f1822a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "original = \"Der schnelle braune Fuchs sprang ueber den faulen Hund\"\n",
    "\n",
    "reference =   \"The quick brown fox jumped over the lazy dog\"\n",
    "candidate_1 = \"The fast  brown fox leaped over the      dog\"\n",
    "candidate_2 = \"The swift brown fox jumped over the lazy dog\"\n",
    "candidate_3 = \"The swift tawny fox leaped over the indolent canine.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3677a7a3-d4d1-49a0-ab90-d02b526cde04",
   "metadata": {},
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a159771-e2ee-4adc-b53d-d349d69a6a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb45c369-e04f-43ea-a31f-e2380efa3e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score for example 1: 0.66\n",
      "BLEU score for example 2: 0.89\n",
      "BLEU score for example 3: 0.44\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "bleu_nltk_1 = sentence_bleu([reference.split()], candidate_1.split(), weights=[1.])\n",
    "bleu_nltk_2 = sentence_bleu([reference.split()], candidate_2.split(), weights=[1.])\n",
    "bleu_nltk_3 = sentence_bleu([reference.split()], candidate_3.split(), weights=[1.])\n",
    "\n",
    "print(f\"BLEU score for example 1: {bleu_nltk_1:.2f}\")\n",
    "print(f\"BLEU score for example 2: {bleu_nltk_2:.2f}\")\n",
    "print(f\"BLEU score for example 3: {bleu_nltk_3:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89def98-71e9-4ee4-96ac-017dd9bf2a28",
   "metadata": {},
   "source": [
    "### TorchMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01198d15-c16c-4aa6-a8ed-65705ab743cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score for example 1: 0.66\n",
      "BLEU score for example 2: 0.89\n",
      "BLEU score for example 3: 0.44\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics import BLEUScore\n",
    "\n",
    "bleu = BLEUScore(n_gram=1)\n",
    "\n",
    "# Calculate BLEU scores\n",
    "bleu_tm_1 = bleu(target=[[reference]], preds=[candidate_1])\n",
    "bleu_tm_2 = bleu(target=[[reference]], preds=[candidate_2])\n",
    "bleu_tm_3 = bleu(target=[[reference]], preds=[candidate_3])\n",
    "\n",
    "print(f\"BLEU score for example 1: {bleu_tm_1:.2f}\")\n",
    "print(f\"BLEU score for example 2: {bleu_tm_2:.2f}\")\n",
    "print(f\"BLEU score for example 3: {bleu_tm_3:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe6a608-168a-4f7c-87a0-d82b9eac21ac",
   "metadata": {},
   "source": [
    "### From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66075150-bb85-465f-945e-3cc008de7efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score for example 1: 0.66\n",
      "BLEU score for example 2: 0.89\n",
      "BLEU score for example 3: 0.44\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "def ngrams(sentence, n):\n",
    "    return [tuple(sentence[i:i+n]) for i in range(len(sentence)-n+1)]\n",
    "\n",
    "def modified_precision(reference, candidate, n):\n",
    "    ref_ngrams = Counter(ngrams(reference, n))\n",
    "    cand_ngrams = Counter(ngrams(candidate, n))\n",
    "\n",
    "    count_clip = sum(min(cand_ngrams[ng], ref_ngrams[ng]) for ng in cand_ngrams)\n",
    "    count_total = sum(cand_ngrams.values())\n",
    "\n",
    "    return count_clip / count_total if count_total > 0 else 0\n",
    "\n",
    "def brevity_penalty(reference, candidate):\n",
    "    ref_len = len(reference)\n",
    "    cand_len = len(candidate)\n",
    "\n",
    "    if cand_len > ref_len:\n",
    "        return 1\n",
    "    elif cand_len == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return math.exp(1 - ref_len / cand_len)\n",
    "\n",
    "def bleu_score_unigram(reference, candidate):\n",
    "    bp = brevity_penalty(reference, candidate)\n",
    "    precision = modified_precision(reference, candidate, n=1)\n",
    "\n",
    "    return bp * precision\n",
    "\n",
    "\n",
    "bleu_scratch_1 = bleu_score_unigram(reference=reference.split(), candidate=candidate_1.split())\n",
    "bleu_scratch_2 = bleu_score_unigram(reference=reference.split(), candidate=candidate_2.split())\n",
    "bleu_scratch_3 = bleu_score_unigram(reference=reference.split(), candidate=candidate_3.split())\n",
    "\n",
    "print(f\"BLEU score for example 1: {bleu_scratch_1:.2f}\")\n",
    "print(f\"BLEU score for example 2: {bleu_scratch_2:.2f}\")\n",
    "print(f\"BLEU score for example 3: {bleu_scratch_3:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
